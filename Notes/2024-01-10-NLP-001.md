---
layout: post
title: "CS6120 : Natural Language Processing | Week - 01"
subtitle: Notes for my NLP class at @NEU
description: Notes for my NLP class at @NEU
categories: lectures
tags: [notes, NLP, CS6149, Computer Science]
toc:
  sidebar: left
---
---



# What is Natural Language?

Natural language is the language that humans utilize to converse with each other in their day to day lives. Natural language can be of spoken and written form. It is a medium through which humans convey their ideas, thoughts, emotions, actions, and exchange information.  Natural language can be thought of a programming language for our life where we interact with various modules to exchange information which allows us to perform tasks. 

Natural language is no single language, but it can be thought of as an umbrella term under which a collection of different languages that have evolved with society and humans themselves fall. Even though these languages may vastly differ from each other, most languages follow certain rules that apply to natural language and have some order to them. 

## Why are computer scientists interested in Natural Language?

Since natural language is the way through which humans communicate and exchange information with each other. It would be incredibly useful to us as a society if machines were able to directly understand what we were saying and obey our commands in the language that we are fluent in natively, eliminating the need for mediums like complicated user interfaces. 

Speaking in natural language abstracts a lot of middleware and would make our lives way easier. Imagine a world where you are able to ask questions to machines as if they were conscious, like talking to your coffee machine or toaster in the morning. This is now partly possible becuase of the advancements in speech recognition. 

## What is Natural Language Processing?

Simply put, natural language processing or NLP is a collection of techniques that focus on making natural human language to be processed and understood by machines and making machines with the goal of them being able to extract information from the language like we do, and make decisions or perform tasks based on the extracted information. It involves techniques that allow us to help machines learn patterns in natural language and learn context. 

Context refers to the circumstances or information that surrounds a particular word, phrase, or statement and helps in determining its meaning. Context can be affected by different parts of the language and information contained by the language. This is the hard part for making machines understand natural language because it can vary so much. However, as we discussed before, language has some core ground rules.



## What are the components of a language?

Language is a complex tool which we use to communicate and it has various components that are required to work together effectively to sustain proper communication. Themain components of a language include:

- **Phonetics** - It is how human percieve sound. It defines the correlation between the written medium and the spoken medium. The way something is spoken is often reflected in the way it is written in languages. For example, in the English language, consonants and vowels are a differentiation of letters based on how they are articulated. They can also be based on the loudness of the voice (acoustic), and based on how similar voices are percieved and differentiated (auditory).

  

- **Morphology** - It is the study of words and the parts of a word, and the relationship of a word to other words. A **morpheme** is the smallest unit of meaning in a language. It can be a word (like cat, rat), or a part of a word(like a prefix, e.g. "un-"). It also studies homophony, where words that sound similar but have different meanings. Morphemes can also be blended together to form another word, like "black" and "board" form blackboard.

  

- **Syntax** - It defines the rules which allows languages to have some consistency and structure to them so that they are interpreted easier. It governs the structure and arrangement of words and phrases so that they make coherent sense. Syntax is thought of as a subdivision of **grammar**, which is the entire set of rules for a language. 

  - Syntax includes things like parts of speech (nouns, adjectives, verbs, adverbs, etc.), phrasing and more. 
  - Sentences and phrases that look and sound similar may have different meanings because of syntactic ambiguity. It happens when there are more than one way of interpreting a given sentence, and the appropriate interpretation is understood only through context. For example, if someone says "Call me a cab.", they might not be telling you to call them as "Cab" instead of their name, but to call a cab to pick them up. This is a major challenge for language models to understand cause it goes beyond basic syntax.




- **Semantics -** Semantics deals with the meaning of words, phrases, sentences and language. Definitions of words as well as their meaning in relation to other words is studied through semantics. 
  - Sense relations: Words with similar meanings (synonyms), opposite meanings (antonyms), and homonyms, hypernyms are studied under semantics.
  - Semantic field :Words that are related to each other and can be classified under a type can be said to be in the same semantic field. For example, roses and daisies are in the semantic field of flowers. 
  - Pragmatics : Pragmatics come into play when we look at how context and culture changes the meaning of a word or sentence. A sentence may have different connotations depending on where and how it is spoken or written. 
  - Semiotics : It is how symbols and colors are associated with certain meanings. For example, red and the skull may mean danger universally. 


# Formal Language

Amongst natural languages, there is a subset of languages with precise sets of rules and symbols which act as a guide on how to write them acceptably. Unlike natural languages, formal languages limit ambiguity and ensure clarity. They entail predictable behavior. Programming languages and mathematical equations can be called examples of formal languages. Formal languages can be used to express precise logic and denote repeatable tasks. This kind of language is much more interpretable by machines because of its predictable nature and defined set of rules on how to write and interpret them.

## Regular Expressions

Regular expressions or regex are sequences of characters which define a specific search pattern. They utilize a formal language called regular grammar. They are the basis of text search, text processing, and other string/literal manipulation tasks. A lot of programming languages have regex libraries for this reason. They can be utilized because a finite state machine is effectively able to process or parse such a language. 

Here is a small introduction to [Regular Expressions](https://www.regular-expressions.info/quickstart.html) if you want to learn more.



# Tokenization

As we discussed, machines do not understand sentences directly. Since sentences are complex, it is a bad idea to feed the machines sentences as information chunks directly because we will then lose the more granular detail in those sentences. Hence it is a good approach to break sentences up into smaller parts. This is called **tokenization**, where we break down text into its component units called **"tokens"**. Tokenization is a crucial step in modern NLP tasks and helps natural language understanding. 

## Types of Tokens

1. **Word-based** **:** This is one of the most naive ideas we have when we think about splitting text into tokens. When we assume words as tokens, we break down a sentence into.. well you guessed it, its component words.

   ​	For example : "The quick brown fox jumps over the lazy dog" would be tokenized as 	["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"].

   To perform word-based tokenization we will be using the popular `nltk` library:

    ```python
    import nltk
    text = "This is a sentence with some words."
    tokens = nltk.word_tokenize(text)
    print(tokens)
    ```

    Output will be `['This', 'is', 'a', 'sentence', 'with', 'some', 'words']`

2. **Phrase-based :** This form of tokenization divides sentences into chunks by grouping words that are semantically and contextually related into a single phrase. This allows the machine to understand which words are related to the same semantic field. 

3. **Morpheme-based :** This form of tokenization assumes that tokens are morphemes, which are the smallest units of a sentence that carry meaning. They are smaller than words. For example, the word "running" can be broken down into the morphemes "run", which is the base form of the verb, and "-ing". Such forms of tokenization can be useful in a process called "**stemming**", which means reducing a word to its root form, as well as sentiment analysis. It is more complex than word based tokenization because it requires the knowledge of the morphological properties of the language. 

4. **Sentence-based :** This form of tokenization is based on splitting large paragraphs into particular sentences. The final token list formed is an array of sentences.

To perform sentence-based tokenization, we can use `nltk` again

```python
import nltk
text = "This is a sentence with some words, including a phrase."
sentences = nltk.sent_tokenize(text)
print(sentences)
```

```
['This is a sentence with some words, including a phrase.', '']
```

5. **n-gram tokens:** This form of tokenization assumes that a token is made of 'n' number of consecutive words. For example, when n=2, also known as **bi-gram**, the sentence is divided into groups of two, and in n=3 (trigram), they will be divided into groups of 3. The purpose of including multiple words is to capture the context between the words, which can be useful for tasks like sentiment analysis. n-gram tokenization is useful but also has certain disadvantages. They are not useful to capture context when n becomes sufficiently large. Hence, n-grams are bad at generalizing context of the entire text or even sentence. They are also a higher dimensional representation of text, hence it can be hard to work with n-gram representation when dealing with a large text corpus. 

We can build a simple n-gram tokenizer using the word tokenizer from `nltk` and then grouping up the words together.

```python
import nltk
from collections import Counter

def n_gram_tokenizer(text, n=1):
    """
    N-gram tokenizer for text.
    
    Parameters:
    text (str): The text to tokenize.
    n (int): The number of items in each n-gram. Default is 1.
        
    Returns:
    list: A list of n-grams found in the text.
    """
    
    # Split the text into individual words
    tokens = nltk.word_tokenize(text)
    
    # Create a dictionary to store each n-gram and its count
    n_gram_dict = Counter()
    
    # Iterate through each window of length n
    for i in range(len(tokens)-n+1):
        # Get the substring at index i and convert it to a tuple
        n_gram = tuple(tokens[i:i+n])
        
        # If the n-gram is not already in the dictionary, add it with a count of 1
        if n_gram not in n_gram_dict:
            n_gram_dict[n_gram] = 1
        # Otherwise, increment the count for that n-gram
        else:
            n_gram_dict[n_gram] += 1
            
    # Convert the dictionary to a list of tuples and return it
    return list(n_gram_dict.items())

```

Here, the function is basically a loop which runs across the token array and then creates a tuple of adjacent tokens.




# Methods of Tokenization

1. **Rule-based** : This involves using a specific rule to break text into tokens. These rules can be regex-based ; as in splitting the text by whitespace or punctuation, or by a word. It is useful when the text is irregular or there are too many grammatical mistakes. This form of tokenization is deliberate and hence is sometimes prone to producing unwanted results. For example, sometimes whitespace-based tokenization may result into a faulty token sequence.

2. **Statistical tokenization :** This method uses probabilistic models to predict where tokens should be placed in a sentence. These models can be trained on large corpora and use statistical patterns to determine word breaks. These approaches work when pattern-based approaches fail.

3. **Machine/Deep learning-based tokenization :** This method involves using deep learning techniques such as recurrent neural networks (RNNs) or long short-term memory (LSTM) networks to automatically segment text into tokens based on syntactic and semantic features. These models have shown to be effective in handling complex sentence structures and contextual dependencies.



# Goals for Week 2

- Complete and submit Assignment 0. 
- Topics to be covered next lecture, we will build upon tokenization :
  - Stemming & Lemmatization, Text Normalization
  - Language Model Estimation, Smoothing Techniques, Zips’ law
  - Rule-Based and Statistical POS Tagging


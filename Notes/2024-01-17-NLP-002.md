---
layout: post
title: "CS6120 : Natural Language Processing | Week - 02"
subtitle: In this lecture we discuss topics like Text Representation Methods, BoW representation, Zipf's Law and Text Normalization.
description: Notes for my NLP class at @NEU
categories: lectures
tags: [Text Normalization, notes, NLP, CS6120, Computer Science]
toc:
  sidebar: left
mermaid: true
---


# Text representation

NLP is all about making machines understand text. Since machines cannot understand text directly and do not speak in natural language but in binary, we need a method to transform or represent the text in a way such that machines can understand them. Such a technique is what call text representation. 

There are various methods to represent text with their own strengths and weaknesses. A few examples are bag-of-words, term frequency-inverse document frequency (TF-IDF), word embeddings, and more.

## Challenges when dealing with represented text

No text representation is perfect, but some are closer than others. Even so, there are different strengths and weaknesses to all of the methods listed above. 

1. Information loss
The biggest challenge that we face when representing text is information loss. A sentence is way more than its semantics and verbiage. Text representation is a form of abstraction, and if it's imperfect in abstracting, information encoded within the sentence can be lost. To capture all the information that is embedded in the sentence is a hard problem, specially since the information gained is often up to interpretation. 

For example when we use Bag of Words or TF-IDF representation, it does little to capture the context and order of the sentence since it only captures the frequency of words in a corpus - but not the semantic and contextual relationship between the words.

2. Vocabulary size and Sparsity
The english language contains about a million words. Representation of such a high amount of words can be really hard, because often it is a higher dimensional representation.

Although our day-to-day lives use a very limited section of this total number, it can still be a considerable amount to invoke the curse of dimensionality when some approaches are used. Some corpora of text like Wikipedia may deal with thousands of unique words.

The curse of dimensionality is a situation when the number of features or dimensions grow exponentially with respect to the size of the dataset, causing sparseness to occur : very high number of features with each feature having very small number of samples. A lot of naive methods like BoW and TF-IDF are subject to falling for this curse because they concern with learning frequencies of each word within a corpora.

3. Ambiguity

Ambiguity refers to a situation where a word, phrase or a whole sentence may have different contextual interpretations or meanings. Ambiguity is common and can arise in natural language due to homonymy, polysemy or when we are concerned with a whole sentence, syntactically where a sentence can be interpreted in multiple ways given no additional context. 

Ambiguity is a major problem in NLP since it can lead to difficulties in the machine understanding the true contextual meaning of the text. For example, consider the word "bank." Without context, this word could refer to a financial institution, the side of a river or lake, or even a type of aircraft. Similarly, the sentence "She gave him her book" could be interpreted as giving away a physical object or selling access to digital content.

Ambiguity can also occur due to the semantic similarity of two pieces of text. Words with similar meanings do not occur in the same order in the sentence, hence making it difficult for a model to generalize the semantic and contextual relationship of such words using traditional approaches like BoW.

To combat this, different NLP models use techniques to remove ambiguity as much as possible. Some of the approaches are dependency parsing and named entity recognition. Modern probabilistic models are better than traditional ones when disambiguating text.

4. Out-of-Vocabulary Words (OOV)
This is a common problem that machine learning approaches face all the time : data that is previously unseen and unheard of. This can happen when the model is trained on a limited dataset or when the dataset does not include all possible variations of a word. OOV words can cause issues in NLP models because they may be difficult to represent or classify accurately.

There are several techniques to deal with this problem, such as subword embeddings, byte-pair encoding and zero-shot approaches.

5. Named Entity recognition (NER)
It is the task of finding and categorizing named entitites inside a corpus of text. Named entitites include entities such as organizations, locations, and people. Named entities are often important contextual keys in sentences. It is hard to recognize named entities because of their contextual placement within sentences changes with different text examples and languages. NER is hence not easily managed by statistical approaches like BoW, but predictive/probabilistic approaches such as vector embeddings may understand underlying patterns.

# One-Hot Vector encoding (Naive Approach)

A naive way of making machines understand the semantic structure of our text corpus is by binary encoding. We can mark the position of each word inside a sentence in a binary vector, where the value of each element corresponds to a word in the sentence, and the value of the element signifies if the word is present at that location in the sentence (1) or not (0). 

Let us try to implement one-hot vector encoding the classic sentence : "The quick brown fox jumps over the lazy dog.".

To do this, we tokenize the sentence by whitespaces using `document.split()`. Then, we can check for unique words by using `set()` which would remove duplicate words. Then, we sort the words in alphabetical order.

We can then produce a numpy matrix of size $$n \times m$$, where $$n$$ is the original number of words, and $$m$ is the number of unique words. Then we simply match each word in the document with our set of unique words and one-hot encode the position of the word inside the matrix.



```python
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

document = "The quick brown fox jumps over the lazy dog."

words = document.split()

unique_words = set(words)

sorted_words = sorted(list(unique_words))

onehot_encoded = np.zeros((len(words), len(unique_words)), int)

for i in range(len(words)):
    onehot_encoded[i, sorted_words.index(words[i])] = 1

print(onehot_encoded)

onehot_representation = pd.DataFrame(onehot_encoded, columns=sorted_words)
print(onehot_representation)
```

This form of representation retains all the information about the grammar and order in a sentence. But note that this results in $$n$$ vectors (given there are $$n$$ words in the corpus) of $$m$$ length because each word in the corpus is represented by a vector of $$m$$ length, which is extremely a extremely high dimension-to-dataset ratio and will easily lead to the curse of dimensionality after adding just a few words/sentences. 

Such a method is ineffective when dealing with millions of tokens in a large text corpus, it will simply take up too much space and increase sparsity. We're talking about $$n^2$$ space complexity, so if you were to store a million bits worth of text (which is around 1500 pages according to [this](https://medium.com/mlearning-ai/the-future-of-language-models-how-a-1-million-token-memory-could-revolutionize-ai-2-2c411267ed7e) source) using this method, you will need a million-million bits - roughly equivalent to 160TB of data.

# Bag-of-Words Representation

The high dimensionality curse that incurred while using one-hot vectors was because we were using multiple dimensions to store the location of each word in a sentence. How important is that semantic location information? Can we get away with using a representation that does not care about the orientation of words? This is exactly what a technique called Bag-of-Words (BoW) explores. 

It is a common method of text representation which just directly stores the unique words in a text corpus and their frequencies. Let's understand that by an example.

"The quick brown fox jumps over the lazy dog" and "The dog barks at the fox." If we represent these documents using bag-of-words model, we would get the following vectors:

```
Document 1: [2, 1, 1, 1, 1, 1] (the = 2, quick=1, brown=1, fox=1, jumps=1, over=1, lazy=1, dog = 1)
Document 2: [1, 1, 1, 1, 1, 1] (The=0, dog=1, barks=0, at=0, fox=0)
```

This at first glance does not make sense and sounds frankly, quite stupid - because then sentences like "The dog cried and cat laughed." and "The dog laughed and the cat cried" would be the exact same sentences since semantic order is not preserved. However, surprisingly enough, if we look at the bag of words representation of a sentence or even a big document, we can get some idea of what exactly the sentence is talking about - even though we may not get the correct context everytime. Similarly, machines can also do that. This technique is very useful when we are comparing two documents. It is also way more efficient than a one-hot-encoded representation of data. In fact, a BoW vector representation can be thought of as all the one-hot vectors added together into a singular vector. Bag of words is often the baseline metric against which other, more advanced techniques are compared. 

Let's see how we can create and utilize BoW vectors - also called document-term matrices or DTMs. Creating DTMs is pretty easy using `CountVectorizer` which can allow us to directly create the DTM from our documents using the `fit_transform()` function. Then we simply put get the number of unique words or features.

```python 
from sklearn.feature_extraction.text import CountVectorizer

documents = [
    "The quick brown fox jumps over the lazy dog",
    "The dog barks at the fox"
]

vectorizer = CountVectorizer()

bow_matrix = vectorizer.fit_transform(documents)

feature_names = vectorizer.get_feature_names()

print(bow_matrix.toarray())

print(feature_names)

```

This will produce the following output:

```
[[0 0 1 1 1 1 1 1 1 2]
[1 1 0 1 1 0 0 0 0 2]]
['at' 'barks' 'brown' 'dog' 'fox' 'jumps' 'lazy' 'over' 'quick' 'the']
```

As we can see, most words in 2 are also in 1, except the word "at" and "barks".

We can already see that the space that is required to store the two documents as DTMs is less (than half!) than the space required to store a single one-hot encoded document! This is why BoW is so efficient - it is linear storage - we require only $$n$$ elements (where $$n$$ is the total number of documents in all the documents fed to BoW) for this representation. 

Still, vanilla BoW representations have problems - they are extremely sparse still. This means there are way too many features and a lot of unique words are not represented enough (since they are too infrequent) - and words like "The" and "and" are more quite more represented. 


# Zipf's Law and Text Normalization

The frequency of occurance of a word in a corpus is in an inverse relationship with its rank in the frequency distribution. This is called Zipf's law, and it is a statistical phenomenon.

Mathematically, we can denote Zipf's Law as:

$$
f(r)= \frac{A}{r^S}
$$

Where $$A$$ is a constant, $$f(r)$$ is the frequency of the word $$r$$, and $$s$$ is the distribution exponent.


Zipf's law dictates that if we rank all words in a corpus by their frequency of occurrence and plot the frequency against the rank, we would observe that the graph follows a power-law distribution with a slope of -1. 
For example, here is a famous plot of Zipf's law for two english texts (source : [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law))

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/ZipfLaw.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

By using Zipf's law, we can estimate that a word is twice as more common in text than the word next in the distribution rank in the same text.

## Stop Words Removal

Zipf's law is directly tied to our sparseness problem - since it can be used to gauge the importance of a word in the corpus, based on its frequency. Words that are in the highest ranks in the Zipf's law plot are generally conjunctive words, like "and" and "the" which do not add specific meaning to the sentence themselves. Such words are known as "Stop Words". Unique words are much less common since they occupy but a small fraction of a sentence usually, and are much less repeated.

Hence it is a good idea to trim out these stop words. There are several lists of stop words built into NLP libraries like `nltk` and `spacy` that can remove these stop word tokens from our corpus. In fact, Stop word removal is one of the several **text normalization** techniques - text normalization is a process of transforming text data into a standard and consistent format to reduce its complexity for analysis and processing.

We can simply import the stopwords corpus and then check our corpus against the stopwords. If any tokens match between the two, we remove them. Here is an implementation of the function to remove stopwords:

```python
def remove_stopwords(doc):
    stop_words = stopwords.words('english')
    return " ".join([word for word in doc.split() if word not in stop_words])

def remove_punctuation(doc):
    return doc.translate(str.maketrans('', '', string.punctuation))

```


## Stemming

Often, we will have words that are actually the extension of the same root word. For example, "running" and "run" are extensions of the same word. The only difference is "ning". However, a vanilla BoW will not know the difference between these two and hence we will end up adding features unnecessarily to our BoW. 

Here is where stemming comes into the picture. Stemming is the process of reducing words into their root form (called the "stem"), by eliminating the inflections on a word. It is another text normalization technique. The goal is to group together the different forms of a single word into one feature, hence reducing dimensionality. Stemming also has both a positive and negative effect on text classification and sentiment analysis tasks. 

There exist different forms of stemmers like Porter Stemmer, Snowball Stemmer, and Lanczos Stemmer. 

### Porter Stemmer

The porter stemmer algorithm was built in mind to enhance the efficiency of information retrieval tasks with limited reference from a stem dictionary, and instead was a heuristic based system which searched for generalized patterns to separate root words from suffixes. This use of heuristics, made porter stemmer faster than several other stemmers. However, using these heuristics it produces morphological variants of words, trading over phonetic accuracy.For example, happily $$\rightarrow$$ happili. Although quite accurate in generalizing, it can too sometimes produce inaccurate results. For example, argument $$\rightarrow$$ argument and is not stemmed. This stemmer tends to remove 'e' from the end of the sentence.


We can use the porter stemmer in the following way by importing it from `nltk.stem`.

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

words = ["argument", "running", "jumped"]

stemmed_words = [stemmer.stem(word) for word in words]

print(f"Original words: {words}")
print(f"Stemmed words: {stemmed_words}")
```

Running this will output:
```
Original words: ['argument', 'running', 'jumped']
Stemmed words: ['argument', 'run', 'jump']
```

### Snowball Stemmer

Snowball stemmer is a family of stemmers, which is faster and more accurate version of the porter stemmer. It also supports multilingual stemming, meanwhile the original Porter stemmer only supported english. Hence it is often called the Porter2 stemmer. It has more hardcore rules than the porter stemmer but it also generalizes better. 

```
ILY  -----> ILI
LY   -----> Nil
SS   -----> SS
S    -----> Nil
ED   -----> E,Nil
```

A snowball stemmer can be implemented similarly to how the porter stemmer is implemented.

### Lancaster Stemmer

The Lancaster stemming algorithm works by analyzing the morphology of the words to reduce them to base form using a set of rules like other stemmers. The unique part about this algorithm is that it instead uses a lookup table to determine the correct stem for the word based on some rules. It is more flexible when handling words than other stemmers and can handle complex and compound words, as well as multilingual text. The lancaster stemmer also trims prefixes from the word aside suffixes.

We can implement the lancaster stemmer in a similar fashion to Porter Stemmer by importing it from `nltk.stem`.

## Lemmatization

Lemmatization is another text normalization techinque to extract the base dictionary form, called a lemma, of a given word. This method is more context sensitive than stemming and allows us to represent the word's meaning more accurately. Lemmatization uses a dictionary, called a morphological analyzer to accordingly decide what lemma to return for what specific word. It can also often differentiate between homonyms and understand context between the word. Like the word "saw" can be reduced to "see" or "saw" based on if it is a verb or a noun.

However, most lemmatizers are costlier than stemmers to operate as they are less efficient. When our goal is to purely improve information retrieval and reduce dimensionality, stemmers often will do a better job quantitatively but lemmatizers would do a better job qualitatively. This means that stemmers are more efficient at compressing different inflection based variations of a word into one single stem, but the quality of reading the sentence may go down. Lemmatizers preserve morphological quality (readability) at the cost of being less efficient and proficient at reducing dimensionality.

Lemmatizers can be divided into 3 types based on the principle they work on:
- Rule-Based Lemmatizers: They use linguistic rules and patterns much alike the Porter stemmer to produce word lemmas with some small combination of dictionary lookup for edge cases.
- Dictionary-Based Lemmatizers: Effective but costly, these lemmatizers utilize dictionaries to look up lexicons pertaining to each word. They are extremely effective on languages with high inflection patterns. An example is the CoreNLP Lemmatizer.
- Statistical/Machine Learning-Based: These lemmatizers learn the pattern of lemmas by being trained on a large amount of textual data and can generalize specific vocabulary.

Let's study how a lemmatizer can reduce words to a lemma by studying some different scenarios:

- 'better' $$\rightarrow$$ 'good'
- 'running' $$\rightarrow$$ 'run'
- 'happily' $$\rightarrow$$ 'happy'

meanwhile with a stemmer these words will become:
- 'better' $$\rightarrow$$ 'better' (does not change)
- 'running' $$\rightarrow$ 'run'
- 'happily' $$\rightarrow$ 'happili'

As we can see, a lemmatizer may do a better job preserving the morphological form of the word as well as the context of the word.

Let's see how we can implement a lemmatizer from `spacy` which utilizes the Wordnet corpus for its lookup.

```python
import spacy

def lemmatize_text(text):
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)
    lemmatized_text = " ".join([token.lemma_ for token in doc])
    return lemmatized_text

text = "The cats are running and jumping over the fences."
lemmatized_text = lemmatize_text(text)
print(lemmatized_text)
```

## Stemming vs Lemmatization

Both stemming and lemmatization are provided as options for reducing the curse of dimensionality and reducing ambiguity - but which one should we use in different scenarios? It may be tempting to use both stemmers and lemmatizers, but often that is not effective because most stemmers are harsher on prefix and suffix removal than lemmatizers. Hence if we used both, we would lose the advantage that lemmatizers have in certain cases. For example, if we lemmatized 'happily' it will be turned to 'happy' but if we applied stemming over it, we will get 'happili' instead, losing the actual lemma.

Let's compare our direct outcomes first. If we want to retain the morphology of the word and its direct contextual meaning, a lemmatizer is a good choice for reducing inflectional forms of a word. Such an instance will be when the meaning of a word directly contributes to the outcome we are searching for. Tasks like sentiment analysis and text based classification where context can be incredibly important in such applications.

On the other hand, if we want to reduce information retrieval latency and the original form of the word is not that important, for tasks like text based indexing, search engines, and text mining - where the outcome is not directly impacted or is at least indirectly abstracted. Stemming is more useful when we want to favor computational efficiency over meaning - and a small feature space is prioritized over semantic accuracy.


## Unicode Normalization

Unicode or Universal Character Set is an encoding system for digital characters that is used to standardize the digital representation of the world's written languages. Unicode uses a number or code to denote each character and hence makes text processing very fast in browsers and NLP. Since it is a standard, it is the same across all platforms.

Often, we would see different representations of text in different font variants like ℌ or ℍ for the letter H. Otherwise, linebreaks and unicode spaces are quite common in text datasets that are scraped and are often treated as tokens (for example the [SEP] token). Unicode is also used to write accent inflections on characters like  é (U+0301) or ñ (U+00F1). We can also see from the example how unicode can be represented as a decimal number that points to a specific value in a lookup table.

However, all such inflections cast doubt since in some areas of a document these inflections may exist, and in some others they may not. Hence it is important to maintain at least some degree of unicode normalization - such as font conversion. 

There are several types of unicode normalization and each deals with its own input-output scenario.

### Normalization Form Decomposition
NFD is a form of Unicode Normalization when characters are decomposed into a sequence of base characters and code marks. For example, the unicode character "â" (U+00E2) can be broken down into "a" (U+0061) + " ̂  ̂" (U+0302). Each character must be fully representable while still using the smallest number of code points.

In python, the `unicodedata` library provides the necessary tools for doing Unicode normalization. In this case, we can use the `normalize()` function in the library to produce results.

```python
import unicodedata as unc


def decompose(text):
    return unc.normalize('NFD', text)

text = "Héllo, hôw àre ýou?"

print(decompose(text))

```

### Normalization Form Composition
In NFC, characters are composed into a precomposed form wherever possible. This in turn results in a single code point representation. This leads to text forms compressed into their precomposed canonical form. It is also viewed as the opposite of NFD. NFD is useful in converting hanging unicodes from copied and scraped text into proper compressed single code format.


We can yet again use `unicodedata` to perform NFC using the `normalize()` function:
```python
import unicodedata as unc

def compose(text):
    return unc.normalize('NFC', text)

text = "e\u0301"

print(compose(text))

```

Running this code will output `é`, the equivalent single code canonical form of the text.

# Next Week
Different String distances
Spell checking algorithms
TF-IDF explanation and more

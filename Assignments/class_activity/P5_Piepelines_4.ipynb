{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 12:00:22.335683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Import pipeline\n",
    "# --------------------\n",
    "from huggingface_hub    import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Example of text generation task in pipeline\n",
    "\n",
    "text_generation_pipeline = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "text = \"Once upon a time\"\n",
    "\n",
    "output = text_generation_pipeline(text, max_length=50, do_sample=False)\n",
    "\n",
    "print(output[0][\"generated_text\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Generate multiple responses\n",
    "# --------------------\n",
    "responses = text_generation_pipeline(\"Once upon a time\", num_return_sequences=3)\n",
    "\n",
    "# Print each generated response\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Response {i+1}: {response['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Even more control over the generated text\n",
    "# ----------------------------------------\n",
    "from transformers import pipeline\n",
    "\n",
    "# ----------------------------------------\n",
    "# Generate text with custom parameters\n",
    "# ----------------------------------------\n",
    "\n",
    "responses = text_generation_pipeline(\"ONce upon a time\",\n",
    "                                     max_length=20,\n",
    "                                     temperature=0.7,# The temperature controls the randomness of the generated text.\n",
    "                                     top_k=50,#Limits the number of highest probability vocabulary tokens considered for each step. A higher top_k increases diversity.\n",
    "                                     top_p=0.95,\n",
    "                                     num_return_sequences=3)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# With top_p, the model considers the set of tokens whose cumulative probability exceeds the specified threshold p.\n",
    "# This means that instead of considering a fixed number of top tokens, it dynamically selects a subset of tokens based on their cumulative probability.\n",
    "# A higher value includes more tokens in the sampling pool, allowing for more diverse token selections. \n",
    "# This can lead to creative text.\n",
    "# A lower value restricts the sampling pool resulting in more focused and deterministic generated text.\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Print each generated response\n",
    "# ----------------------------------------\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Response {i+1}: {response['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with the hyperparameters to complete the sentence. Here are some examples:\n",
    "\n",
    "\n",
    "- She opened the ancient book and discovered\n",
    "\n",
    "- In a world where robots\n",
    "\n",
    "- The secret to happiness is\n",
    "\n",
    "- The hidden treasure was finally\n",
    "\n",
    "- As the sun set over the horizon\n",
    "\n",
    "- The star player took a deep breath, lined up the shot, and\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
